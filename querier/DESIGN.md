# CS50 TSE Crawler

Author: Amittai J. Wekesa (github: @siavava)

***

## Design Spec

In this document we reference the [Requirements Specification](REQUIREMENTS.md) and focus on the implementation-independent design decisions.
T
Design considerations:

- User interface
- Inputs and outputs
- Functional decomposition into modules
- Data flow through modules
- Pseudo code (plain English-like language) for logic/algorithmic flow
- Major data structures
- Testing plan

***

## User interface

As described in the [Requirements Spec](REQUIREMENTS.md), the querier's only interface with the user is on the command-line; it must always have two arguments.

``` bash
./querier pageDirectory indexFilename
```

`pageDirectory` is the pathname to a directory created by the [Crawler](../crawler/README.md).
`indexFilename` ia the path to a file created by the [Indexer](../indexer/README.md).

***

## Inputs and outputs

### Input

1. On initialization, the Querier requires two inputs as listed above: a path to a directory generated by the crawler, and a path to an index file produced by the indexer.

2. During runtime, the querier reads search queries from `stdin`, one per line, until EOF.

### Output

1. The Querier prints cleaned-up queries for the user before running them.

2. The Querier prints query results to `stdout`, ranked by scores; this listing includes:

    - the page's score.

    - the document ID wherein the page is stored.

    - the page's URL.

3. The Querier prints to `stderr` whenever an exception encountered.

4. On termination, the Querier returns a code dependent on the condition that prompted an exit:

***

## Functional decomposition into modules

The Querier is composed of the following functions:

 1. *main*, which parses arguments, ascertains that the correct number of arguments were received, and initializes other modules.
 2. *parseArgs*, which receives allocated memory from main and saves therein the values passed from `stdin`.
 3. *buildIndex*, which rebuilds the `index` structure from the index file.
 4. *getQuery*, which prompts user for queries and normalizes them.
 5. *query*, which checks for existence of the query in the index.
 6. *quit*, which cleans up and exits the application.

And some helper modules that provide data structures:

 1. *index* of pages and word occurrences.

***

## Pseudo code for logic/algorithmic flow

The querier will run as follows:

```pseudocode
confirm correct number of arguments was received from command line
parse input and validate Parameters
reconstruct the index from specified file
prompt user for query
while valid query received,
  process the query
  prompt for new query

  parse the command line, validate parameters, initialize other modules
  add seedURL to the bag of webpages to crawl, marked with depth=0
  add seedURL to the hashtable of URLs seen so far
  while there are more webpages in the bag:
      extract a webpage (URL,depth) item from the bag
      pause for one second
      use pagefetcher to retrieve a webpage for that URL
      use pagesaver to write the webpage to the pageDirectory with a unique document ID
      if the webpage depth is < maxDepth, explore the webpage to find the links it contains:
        use pagescanner to parse the webpage to extract all its embedded URLs
        for each extracted URL:
          normalize the URL (per requirements spec)
          if that URL is internal (per requirements spec):
            try to insert that URL into the *hashtable* of URLs seen;
              if it was already in the table, do nothing;
              if it was added to the table:
                  create a new webpage for that URL, marked with depth+1
                  add that new webpage to the bag of webpages to be crawled
```

Notice that our pseudocode says nothing about the order in which it crawls webpages.
Recall that our *bag* abstract data structure explicitly denies any promise about the order of items removed from a bag.
That's ok.
The result may or may not be a Breadth-First Search, but for the crawler we don't care about the order as long as we explore everything within the `maxDepth` neighborhood.

The crawler completes and exits when it has nothing left in its *bag* - no more pages to be crawled.
The maxDepth parameter indirectly determines the number of pages that the crawler will retrieve.

## Major data structures

Helper modules provide all the data structures we need:

- *bag* of webpage (URL, depth) structures
- *hashtable* of URLs
- *webpage* contains all the data read for a given webpage, plus the URL and the depth at which it was fetched

## Testing plan

We've established a '[playground](http://cs50tse.cs.dartmouth.edu/tse/)' for CS50 crawlers to explore.

A sampling of tests that should be run:

1. Test the program with various forms of incorrect command-line arguments to ensure that its command-line parsing, and validation of those parameters, works correctly.

2. Crawl a simple, closed set of cross-linked web pages like [letters](http://cs50tse.cs.dartmouth.edu/tse/letters/), at depths 0, 1, 2, or more.
Verify that the files created match expectations.

3. Repeat with a different seed page in that same site.
If the site is indeed a graph, with cycles, there should be several interesting starting points.

4. Point the crawler at one of our bigger playgrounds.
Explore at depths 0, 1, 2, from various starting pages.
(It takes a long time to run at depth 2 or higher!)
Verify that the files created match expectations.

5. When you are confident that your crawler runs well, test it with a greater depth - but be ready to kill it if it seems to be running amok.
